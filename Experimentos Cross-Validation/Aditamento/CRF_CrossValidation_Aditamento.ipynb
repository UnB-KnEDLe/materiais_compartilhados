{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gpwowf_F0Wk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b39201-4fce-4540-b8bb-4546a0a9bb57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (4.64.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 2.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.8 sklearn-crfsuite-0.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn<0.24\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.21.6)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.23.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn_crfsuite\n",
        "!pip install -U 'scikit-learn<0.24'\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "e02HRRPzDiX-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6fy0XFU-zo26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e54811-6586-4134-bdf9-4b7de46c0e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import sklearn_crfsuite\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import sklearn\n",
        "import joblib\n",
        "import nltk\n",
        "\n",
        "from sklearn_crfsuite.metrics import flat_classification_report, flat_f1_score\n",
        "from sklearn.metrics import classification_report, make_scorer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn_crfsuite import scorers\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_aditamento = pd.read_csv('https://raw.githubusercontent.com/brunoedcf/data_crf_training/main/aditamento_contratual.csv')"
      ],
      "metadata": {
        "id": "oYP49ky0vEV5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CRF_CrossValidation_Aditamento():\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "    self.labels = [\n",
        "        'B-PROCESSO', \n",
        "        'B-NUM_ADITIVO', \n",
        "        'I-NUM_ADITIVO', \n",
        "        'B-NUM_AJUSTE', \n",
        "        'B-CONTRATANTE', \n",
        "        'I-CONTRATANTE', \n",
        "        'B-OBJ_ADITIVO', \n",
        "        'I-OBJ_ADITIVO', \n",
        "        'B-DATA_ESCRITO', \n",
        "        'I-DATA_ESCRITO', \n",
        "        'B-NOME_RESPONSAVEL', \n",
        "        'I-NOME_RESPONSAVEL', \n",
        "        'B-CODIGO_SIGGO', \n",
        "        'I-NUM_AJUSTE', \n",
        "        'I-PROCESSO'\n",
        "      ]\n",
        "\n",
        "\n",
        "    self.metrics = None\n",
        "    self.crf = None\n",
        "\n",
        "  def get_features(self, sentence):\n",
        "        \n",
        "        sent_features = []\n",
        "        for i in range(len(sentence)):\n",
        "            # print(sentence[i])\n",
        "            word_feat = {\n",
        "                # Palavra atual\n",
        "                'word': sentence[i].lower(),\n",
        "                'capital_letter': sentence[i][0].isupper(),\n",
        "                'all_capital': sentence[i].isupper(),\n",
        "                'isdigit': sentence[i].isdigit(),\n",
        "                # Uma palavra antes\n",
        "                'word_before': '' if i == 0 else sentence[i-1].lower(),\n",
        "                'word_before_isdigit': '' if i == 0 else sentence[i-1].isdigit(),\n",
        "                'word_before_isupper': '' if i == 0 else sentence[i-1].isupper(),\n",
        "                'word_before_istitle': '' if i == 0 else sentence[i-1].istitle(),\n",
        "                # Uma palavra depois\n",
        "                'word_after': '' if i+1 >= len(sentence) else sentence[i+1].lower(),\n",
        "                'word_after_isdigit': '' if i+1 >= len(sentence) else sentence[i+1].isdigit(),\n",
        "                'word_after_isupper': '' if i+1 >= len(sentence) else sentence[i+1].isupper(),\n",
        "                'word_after_istitle': '' if i+1 >= len(sentence) else sentence[i+1].istitle(),\n",
        "\n",
        "                'BOS': i == 0,\n",
        "                'EOS': i == len(sentence)-1\n",
        "            }\n",
        "            sent_features.append(word_feat)\n",
        "        return sent_features\n",
        "\n",
        "  def load(self, data_frame):\n",
        "\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "\n",
        "    for i, row in enumerate(data_frame['treated_text']):\n",
        "      self.x.append(word_tokenize(data_frame['treated_text'][i]))\n",
        "      self.y.append(data_frame['IOB'][i].split())\n",
        "\n",
        "    for i in range(len(self.x)):\n",
        "        self.x[i] = self.get_features(self.x[i])\n",
        "\n",
        "\n",
        "  def optimize(self):\n",
        "\n",
        "    crf_optimizer = sklearn_crfsuite.CRF(\n",
        "      algorithm='lbfgs',\n",
        "      max_iterations=100,\n",
        "      all_possible_transitions=True\n",
        "    )\n",
        "\n",
        "    params_space = {\n",
        "      'c1': scipy.stats.expon(scale=0.1),\n",
        "      'c2': scipy.stats.expon(scale=0.1),\n",
        "    }\n",
        "    f1_scorer = make_scorer(\n",
        "        flat_f1_score,\n",
        "        average='weighted', \n",
        "        labels=self.labels\n",
        "    )\n",
        "\n",
        "    rs = RandomizedSearchCV(\n",
        "        crf_optimizer,\n",
        "        params_space,\n",
        "        cv=10,\n",
        "        verbose=1,\n",
        "        n_jobs=-1,\n",
        "        n_iter=30,\n",
        "        scoring=f1_scorer\n",
        "    )\n",
        "\n",
        "    rs.fit(self.x, self.y)\n",
        "    \n",
        "    print('best params:', rs.best_params_)\n",
        "    print('best CV score:', rs.best_score_)\n",
        "\n",
        "    res = pd.DataFrame(rs.cv_results_)\n",
        "    res.to_csv('results.csv')"
      ],
      "metadata": {
        "id": "htmcn77hvEV5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CRF_CrossValidation_Aditamento()\n",
        "model.load(df_aditamento)"
      ],
      "metadata": {
        "id": "XrWG4Ix2vEV6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.optimize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27066aa6-da92-413d-9c99-02011c614f8c",
        "id": "JovhZu8AvEV6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 68.0min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 284.5min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 432.3min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best params: {'c1': 0.20305452976423158, 'c2': 0.03200481159541715}\n",
            "best CV score: 0.88516906365235\n"
          ]
        }
      ]
    }
  ]
}