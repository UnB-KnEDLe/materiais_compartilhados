{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpwowf_F0Wk7"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn_crfsuite\n",
        "!pip install -U 'scikit-learn<0.24'\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "e02HRRPzDiX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fy0XFU-zo26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03117ef3-e0a5-4bb4-bc2c-2bd5962a9c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import sklearn_crfsuite\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import sklearn\n",
        "import joblib\n",
        "import nltk\n",
        "\n",
        "from sklearn_crfsuite.metrics import flat_classification_report, flat_f1_score\n",
        "from sklearn.metrics import classification_report, make_scorer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn_crfsuite import scorers\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_licitacao = pd.read_csv('https://raw.githubusercontent.com/brunoedcf/data_crf_training/main/licitacao.csv')"
      ],
      "metadata": {
        "id": "oYP49ky0vEV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CRF_CrossValidation_Licitacao():\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "    self.labels = [\n",
        "        'B-NUM_LICITACAO', \n",
        "        'B-CODIGO_SISTEMA_COMPRAS', \n",
        "        'B-ORGAO_LICITANTE', \n",
        "        'I-ORGAO_LICITANTE', \n",
        "        'B-OBJ_LICITACAO', \n",
        "        'I-OBJ_LICITACAO', \n",
        "        'B-PROCESSO', \n",
        "        'B-VALOR_ESTIMADO', \n",
        "        'B-DATA_ABERTURA', \n",
        "        'B-SISTEMA_COMPRAS', \n",
        "        'B-NOME_RESPONSAVEL', \n",
        "        'I-NOME_RESPONSAVEL', \n",
        "        'B-MODALIDADE_LICITACAO', \n",
        "        'I-MODALIDADE_LICITACAO', \n",
        "        'I-SISTEMA_COMPRAS', \n",
        "        'I-DATA_ABERTURA', \n",
        "        'I-PROCESSO', \n",
        "        'B-TIPO_OBJ', \n",
        "        'I-NUM_LICITACAO', \n",
        "        'I-TIPO_OBJ', \n",
        "        'I-VALOR_ESTIMADO', \n",
        "        'I-CODIGO_SISTEMA_COMPRAS'\n",
        "    ]\n",
        "\n",
        "\n",
        "    self.metrics = None\n",
        "    self.crf = None\n",
        "\n",
        "  def get_features(self, sentence):\n",
        "        \n",
        "        sent_features = []\n",
        "        for i in range(len(sentence)):\n",
        "            # print(sentence[i])\n",
        "            word_feat = {\n",
        "                # Palavra atual\n",
        "                'word': sentence[i].lower(),\n",
        "                'capital_letter': sentence[i][0].isupper(),\n",
        "                'all_capital': sentence[i].isupper(),\n",
        "                'isdigit': sentence[i].isdigit(),\n",
        "                # Uma palavra antes\n",
        "                'word_before': '' if i == 0 else sentence[i-1].lower(),\n",
        "                'word_before_isdigit': '' if i == 0 else sentence[i-1].isdigit(),\n",
        "                'word_before_isupper': '' if i == 0 else sentence[i-1].isupper(),\n",
        "                'word_before_istitle': '' if i == 0 else sentence[i-1].istitle(),\n",
        "                # Uma palavra depois\n",
        "                'word_after': '' if i+1 >= len(sentence) else sentence[i+1].lower(),\n",
        "                'word_after_isdigit': '' if i+1 >= len(sentence) else sentence[i+1].isdigit(),\n",
        "                'word_after_isupper': '' if i+1 >= len(sentence) else sentence[i+1].isupper(),\n",
        "                'word_after_istitle': '' if i+1 >= len(sentence) else sentence[i+1].istitle(),\n",
        "\n",
        "                'BOS': i == 0,\n",
        "                'EOS': i == len(sentence)-1\n",
        "            }\n",
        "            sent_features.append(word_feat)\n",
        "        return sent_features\n",
        "\n",
        "  def load(self, data_frame):\n",
        "\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "\n",
        "    for i, row in enumerate(data_frame['treated_text']):\n",
        "      self.x.append(word_tokenize(data_frame['treated_text'][i]))\n",
        "      self.y.append(data_frame['IOB'][i].split())\n",
        "\n",
        "    for i in range(len(self.x)):\n",
        "        self.x[i] = self.get_features(self.x[i])\n",
        "\n",
        "\n",
        "  def optimize(self):\n",
        "\n",
        "    crf_optimizer = sklearn_crfsuite.CRF(\n",
        "      algorithm='lbfgs',\n",
        "      max_iterations=100,\n",
        "      all_possible_transitions=True\n",
        "    )\n",
        "\n",
        "    params_space = {\n",
        "      'c1': scipy.stats.expon(scale=0.1),\n",
        "      'c2': scipy.stats.expon(scale=0.1),\n",
        "    }\n",
        "    f1_scorer = make_scorer(\n",
        "        flat_f1_score,\n",
        "        average='weighted', \n",
        "        labels=self.labels\n",
        "    )\n",
        "\n",
        "    rs = RandomizedSearchCV(\n",
        "        crf_optimizer,\n",
        "        params_space,\n",
        "        cv=10,\n",
        "        verbose=1,\n",
        "        n_jobs=-1,\n",
        "        n_iter=50,\n",
        "        scoring=f1_scorer\n",
        "    )\n",
        "\n",
        "    rs.fit(self.x, self.y)\n",
        "    \n",
        "    print('best params:', rs.best_params_)\n",
        "    print('best CV score:', rs.best_score_)\n",
        "\n",
        "    res = pd.DataFrame(rs.cv_results_)\n",
        "    res.to_csv('results.csv')"
      ],
      "metadata": {
        "id": "htmcn77hvEV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CRF_CrossValidation_Licitacao()\n",
        "model.load(df_licitacao)"
      ],
      "metadata": {
        "id": "XrWG4Ix2vEV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.optimize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db79e6c9-cc63-4d87-c5bb-67248d076b4a",
        "id": "JovhZu8AvEV6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 29.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 123.4min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 280.0min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 313.6min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best params: {'c1': 0.09966202629801069, 'c2': 0.0899064010119305}\n",
            "best CV score: 0.9405374208229647\n"
          ]
        }
      ]
    }
  ]
}